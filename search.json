[{"title":"CNN","url":"/2025/09/24/CNN/","content":"\n参考文章：https://zhuanlan.zhihu.com/p/494796637\n\n背景\n参数共享：图像需要处理的数据量太大，如果直接使用全连接层进行图像特征的提取，参数量会非常巨大，而且很容易过拟合。\n平移不变：假设图像中有一个圆形，如果采用传统的方法进行特征表示，圆形在左上角和在右下角得到的差异会非常大，而从视觉的角度来看，图像的内容并没有变化，只是其位置发生了变化。CNN 解决了这个问题，能够更加有效地保留图像的特征。\n人类视觉：CNN 与人类视觉的原理类似：从原始信号输入开始（像素），随后做初步处理（边缘、方向），然后进行抽象（形状），最后做出判断。CNN 类似于模仿人类大脑的这一过程，较低层识别初级图像特征，若干底层特征组成更上一层的特征，逐步向上最终做出分类。\n与传统的全连接网络不同的是，当前层的神经元只与上一层的部分神经元连接，连接通过卷积运算实现。\n\n模型结构\n\n输入层：三维矩阵的长和宽表示图像的大小，深度表示图像的色彩通道。\n卷积层：将图像的局部特征组合成抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵深度会增加，即通过学习不同的卷积核来组合成更多不同的高级特征。\n池化层：不会改变矩阵深度，只会减小矩阵大小，类似于降采样。主要用于减少最后全连接层中节点的个数，从而减少整个神经网络的参数。\n全连接层：经过卷积层和池化层的处理后，图像信息已经被抽象成信息密度更高的特征，卷积和池化的过程可以理解为自动提取图像特征的过程。全连接层通过对所有特征进行打分和组合，最终实现分类和回归任务。\nsoftmax层：用于分类问题，将全连接层的结果转换为概率。$$  S_i &#x3D; \\frac{e^i}{\\sum_j{e^j}}$$\n\n池化层的反向传播\n由于池化层没有参数参与正向传播过程，所以它是不可导的，无法直接进行反向传播。由于没有参数参与，因此一定要保证分配前后的梯度总和保持不变。\n常用的池化包括：\n最大池化：反向传播时将梯度直接传给最大值神经元\n平均池化：反向传播时将梯度平均分配给各个神经元\n\n\n\n","categories":["AI","神经网络"]}]