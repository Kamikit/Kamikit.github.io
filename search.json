[{"title":"CNN","url":"/2025/09/24/CNN/","content":"\n参考文章：https://zhuanlan.zhihu.com/p/494796637\n\n背景\n参数共享：图像需要处理的数据量太大，如果直接使用全连接层进行图像特征的提取，参数量会非常巨大，而且很容易过拟合。\n平移不变：假设图像中有一个圆形，如果采用传统的方法进行特征表示，圆形在左上角和在右下角得到的差异会非常大，而从视觉的角度来看，图像的内容并没有变化，只是其位置发生了变化。CNN 解决了这个问题，能够更加有效地保留图像的特征。\n人类视觉：CNN 与人类视觉的原理类似：从原始信号输入开始（像素），随后做初步处理（边缘、方向），然后进行抽象（形状），最后做出判断。CNN 类似于模仿人类大脑的这一过程，较低层识别初级图像特征，若干底层特征组成更上一层的特征，逐步向上最终做出分类。\n与传统的全连接网络不同的是，当前层的神经元只与上一层的部分神经元连接，连接通过卷积运算实现。\n\n模型结构\n\n输入层：三维矩阵的长和宽表示图像的大小，深度表示图像的色彩通道。\n卷积层：将图像的局部特征组合成抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵深度会增加，即通过学习不同的卷积核来组合成更多不同的高级特征。\n池化层：不会改变矩阵深度，只会减小矩阵大小，类似于降采样。主要用于减少最后全连接层中节点的个数，从而减少整个神经网络的参数。\n全连接层：经过卷积层和池化层的处理后，图像信息已经被抽象成信息密度更高的特征，卷积和池化的过程可以理解为自动提取图像特征的过程。全连接层通过对所有特征进行打分和组合，最终实现分类和回归任务。\nsoftmax层：用于分类问题，将全连接层的结果转换为概率。$$  S_i &#x3D; \\frac{e^i}{\\sum_j{e^j}}$$\n\n池化层的反向传播\n由于池化层没有参数参与正向传播过程，所以它是不可导的，无法直接进行反向传播。由于没有参数参与，因此一定要保证分配前后的梯度总和保持不变。\n常用的池化包括：\n最大池化：反向传播时将梯度直接传给最大值神经元\n平均池化：反向传播时将梯度平均分配给各个神经元\n\n\n\n","categories":["AI","神经网络"]},{"title":"RNN、LSTM和GRU","url":"/2025/09/24/RNN%E3%80%81LSTM%E5%92%8CGRU/","content":"\n参考文章：\nhttps://zhuanlan.zhihu.com/p/123211148\nhttps://zhuanlan.zhihu.com/p/491564016\nhttps://zhuanlan.zhihu.com/p/32085405\nhttps://zhuanlan.zhihu.com/p/32481747\nhttps://zybuluo.com/hanbingtao/note/541458\nhttps://zybuluo.com/hanbingtao/note/581764\n\n\n\nRNN背景\n对于同一个单词，它在不同句子中的含义可能不同，传统的神经网络模型无法提取其在序列中的特征，预测的准确程度仅取决于训练集中label的数量。\n\n模型结构\n\n输入层：X 是一个向量，表示输入信息，与隐藏层之间不是全连接的，而是按照时刻与隐藏层对齐连接（例如输入一段英文，每个单词对应的向量与隐藏层对齐连接）\n\n隐藏层：S 是一个向量，表示隐藏层的值（节点数与 S 的维度相同）\n\n输出层：O 是一个向量，表示输出层的值\n\n模型参数：U 是输入层到隐藏层的权重矩阵，V 是隐藏层到输出层的权重矩阵，W 是隐藏层到隐藏层的权重矩阵，其中 V 和 W 都是全连接的，且各个时刻的 U、V 和 W 都共享各自的参数\n\n计算过程：$$  S_t &#x3D; f(U \\cdot X_t + W \\cdot S_{t-1})\\  O_t &#x3D; g(V \\cdot S_t)$$\n\n其中 f 和 g 为激活函数\n\n\n\n缺点\n由于整个计算过程都是顺序的，每次输入一个 embedding 顺序计算，后面得到的计算结果包含序列前面的 token 信息，但是前面得到的计算结果并不包含序列后面的 token 信息，因此表达能力有限。\n这种最基础的 RNN 存在梯度消失和梯度爆炸的问题，因此这种最朴素的 RNN 通常不会使用。\n\nRNN 梯度爆炸和梯度消失问题\n简单来说，RNN 梯度消失就是在反向传播过程中，梯度的计算中包含了矩阵的连乘，如果这些矩阵的特征值小于 1，则梯度会指数衰减，最终导致靠近输入层的梯度几乎为 0，参数得不到更新。梯度爆炸与梯度消失类似，如果这些矩阵的特征值大于 1，则梯度会指数增长，从而导致模型更新过大，无法收敛。\n具体解析如下：\n在 RNN 中，更新隐状态的矩阵 $W_h$ 参与了所有时刻隐状态的更新，因此损失对其的梯度为：  $$  \\frac{\\partial L}{\\partial W_h} &#x3D; \\sum_{t &#x3D; 1}^T\\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_h}  $$\n其中 $\\frac{\\partial h_t}{\\partial W_h}$ 可以直接求导得到。\n对于前项 $\\frac{\\partial L}{\\partial h_t}$，可以通过递推公式求得（默认激活函数为 tanh）：  $$  \\frac{\\partial L}{\\partial h_t} &#x3D; \\sum_{k &#x3D; t}^T(\\frac{\\partial L}{\\partial h_k} \\prod_{j &#x3D; t + 1}^k W_h^T \\cdot diag(1 - h_j^2))  $$\n可以看到，这一部分的梯度包含了矩阵连乘，这也就是梯度爆炸和梯度消失的根源。\n该递推式可以如下理解：假设当前时刻为 $t$，那么 $h_t$ 会参与 $t$ 时刻之后的所有隐状态更新，因此 $t$ 时刻的梯度需要对后续时刻的梯度求和得到。递推式的边界条件即为最后时刻的梯度，可以根据损失函数直接求导得到。\n\n\n\nLSTM\n为了解决 RNN 的梯度消失和梯度爆炸问题，提出了 LSTM（Long Short Term Memory Network）。\n原先的 RNN 隐藏层只有一个状态 h（hidden），其对于短期的输入非常敏感，因此 LSTM 的核心思路就是再添加一个隐藏层状态 c（cell），其在向前传递的过程中变化很慢，用于保存长期的状态。\nLSTM 引入了三个门控单元，用于控制 c 的状态与其对输出的贡献。**遗忘门（forget gate）**决定了上一时刻的 cell state $c_{t-1}$有多少会保留到当前时刻 $c_t$；**输入门（input gate）**决定了当前时刻网络的输入 $x_t$ 有多少会保存到 $c_t$；**输出门（output gate）**控制 $c_t$ 有多少输出到 $h_t$。\n其具体结构与计算过程如下图所示：\n首先 LSTM 会先用当前输入 $x^t$ 和上一个状态传递下来的 $h^{t-1}$ 拼接并计算得到 4 个状态，其中 $z$ 是输入，$z^f, z^i, z^o$ 是三个门控单元，分别是遗忘门、输入门和输出门。$$  z &#x3D; tanh(W \\cdot [x_t, h_{t-1}] + b) \\  z^f &#x3D; sigmoid(W_f \\cdot [x_t, h_{t-1}] + b_f) \\  z^i &#x3D; sigmoid(W_i \\cdot [x_t, h_{t-1}] + b_i) \\  z^o &#x3D; sigmoid(W_o \\cdot [x_t, h_{t-1}] + b_o) \\$$\n如上图所示，LSTM 的内部计算主要包含三个阶段：\n忘记阶段：该阶段主要是对上一时刻的 cell 进行选择性遗忘，舍弃不重要信息，保留重要信息，这个过程由忘记门控 $z^f$ 控制。\n选择记忆阶段：该阶段会对当前时刻输入 $z$ 进行选择性记忆，这个过程由输入门控 $z^i$ 控制。  $$  c_t &#x3D; z^f \\odot c_{t-1} + z^i \\odot z  $$\n输出阶段：该阶段决定哪些 cell 的状态会作为当前状态输出，这个过程由输出门控 $z^o$ 进行控制。  $$  h_t &#x3D; z^o \\odot tanh(c_t)  $$\n\n\n最终网络的输出与 RNN 类似：$$  y_t &#x3D; sigmoid(W^\\prime \\cdot h_t)$$\n\n为何 LSTM 能够解决 RNN 的梯度爆炸和梯度消失问题\nRNN 的梯度需要通过隐状态传递，从而出现了矩阵连乘形式，而 LSTM 通过引入 cell state 和遗忘门控使得梯度可以通过这条稳定的路径传递，只要有了这条梯度不会消失和爆炸的传递路径，我们就可以有效地更新模型的参数。\nLSTM 的 cell state 更新如下：$$  c_t &#x3D; f_t \\odot c_{t-1} + i_t \\odot z$$\n对于 RNN 的隐状态更新，$\\frac{\\partial h_t}{\\partial h_{t-1}}$ 可能与激活函数的导数相关，无法保证其稳定在 1 附近。\nLSTM 通过引入遗忘门控来保证 cell state 这条路径可以稳定传递梯度，即：$$  \\frac{\\partial c_t}{\\partial c_{t-1}} &#x3D; f_t \\approx 1$$\n\n缺点\n相较于最朴素的 RNN，LSTM 引入了很多内容，导致参数变多，使得训练难度加大了很多，因此通常会使用效果和 LSTM 效果相当但参数更少的 GRU 来构建大模型。\n\nGRU\nGRU 的结构如图所示：\n\nGRU 引入了两个门控单元，其中 r 为重置门（reset gate），z 为更新门（update gate）：$$  r_t &#x3D; sigmoid(W_r \\cdot [x_t, h_{t-1}])  z_t &#x3D; sigmoid(W_z \\cdot [x_t, h_{t-1}])$$\n\n首先 GRU 会通过重置门将新的输入信息与前面的记忆相结合，具体公式如下：$$  h^\\prime &#x3D; tanh(W \\cdot [x_t, r_t \\odot h_{t-1}])$$\n\n$r_t$ 越小，则前面的记忆占比越小，表示上一时刻的记忆需要丢弃的越多；反之，$r_t$ 越大，则说明上一时刻需要记住的越多。因此重置门是对前一时刻的记忆进行一定的重置，有助于提取序列中短期的关系。\n\n随后 GRU 会进行遗忘和选择记忆，即更新记忆，具体公式如下：$$  h_t &#x3D; (1 - z_t) \\odot h_{t-1} + z_t \\odot h^\\prime$$\n\n$(1 - z_t) \\odot h_{t-1}$ 表示对原本隐藏状态的选择性遗忘，$z_t$ 越大，表示遗忘的更多，剔除记忆中不重要的信息。\n\n$z_t \\odot h^\\prime$表示对当前节点的信息 $h^\\prime$ 进行选择性记忆，$z_t$ 越大，表示保存到记忆中的信息越多。\n\n\n","categories":["AI","神经网络"]}]