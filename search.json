[{"title":"FourierFeatureMapping","url":"/2025/09/24/Embedding/FourierFeatureMapping/","content":"（待完善）\n在生成 Embedding 时的应用\n例如使用三角形 mesh 顶点的三维空间信息进行位置编码时，可以将三个顶点的三维坐标先拼接为 9D 向量，随后将每个分量复制 6 份，这 6 份分量分别乘上相应的频率信息（[1.0, 1.3797, 1.9037, 2.6265, 3.6239, 5.0]），得到一个包含不同频率空间特征的 54 维向量。\n如果需要学习相对位置信息，则可以对这 54 维向量的每一个分量求 sin 与 cos，即 54 个旋转矩阵，从而可以用于RoPE编码（覆盖 embedding 的前 108 维）\n如何理解学习不同频率的空间特征：\n小频率可以表示大尺度的特征，如三角形的全局位置\n大频率可以表示小尺度的特征，如高频细节、边界、形状等\n通过将原始数据乘上不同频率值，可以提取到更多特征信息，从而使模型能够表达更加复杂的函数。\n\n\n\n","categories":["AI","Embedding"]},{"title":"RoPE","url":"/2025/09/24/Embedding/RoPE/","content":"（待完善）\n基本概念：\ntransformer 本身是序列无关模型（self-attention 只看 token 间的相似度，并没有顺序的概念），为了引入位置信息，因此才需要引入位置编码。\n\n绝对位置编码：\n例如序列中第 i 个 token w_i，随后转换为一个 d 维 embedding x_i，为了引入索引 i 这个位置信息，在计算通常会按照一定的编码规则将其转换为一个同样是 d 维的向量 p_i，将embedding与位置编码相加后（x_i + p_i）再计算该 token 对应的 Q、K、V。\n\n最常用的绝对位置编码计算方式如下：\n\n  p_{i,2t} = \\sin{\\frac{i}{10000^{\\frac{2t}{d}}}} \\\\\n  p_{i,2t+1} = \\cos{\\frac{i}{10000^{\\frac{2t}{d}}}}\n其中第一个式子为偶数分量的计算公式，第二个式子为奇数分量的计算公式。\n\n这种编码方式无法保存相对位置关系。\n\n\n相对位置编码：\nRPE 直接在 attention 分数里加上了一个与相对位置（i-j）相关的参数：\n  score(i, j) = Q_i K^T_j + b_{i-j}\n\nRoPE：\n为了能够利用上 token 之间的相对位置信息，我们假定 query 向量 q_m 和 key 向量 k_n 之间的内积可以使用一个函数 g 来表示，其输入为 embedding x_m、x_n 和它们之间的相对位置 m-n：\n\n   = g(x_m, x_n, m-n)\n这样就能够直接将相对位置信息自然嵌入到 attention score 中，我们只需要找到一个等价的位置编码方式来使得上面的等式成立即可。\n\n论文中提到的 f 和 g 形式如下：\n\n  f_q(x_m, m) = (W_q x_m)e^{im\\theta}\\\\\n  f_k(x_n, n) = (W_k x_n)e^{in\\theta}\\\\\n  g(x_m, x_n, m-n) = Re[(W_qx_m)(W_kx_n)^*e^{i(m-n)\\theta}]\n其中 * 表示复数的共轭。\n\n推导过程\n\n最终得到的结果如下：\n\n  Q^{RoPE}_m = R(m) Q_m\\\\\n  K^{RoPE}_n = R(n) K_n\\\\\n  Q^{RoPE}_mK^{RoPE}_n = Q_m R(m-n) K_n^T\n可以看到最终的 attention score 已经包含了相对位置信息。\n\n对于 d 维的 embedding 向量，可以将 embedding 向量分量两两一组分组，每组应用相同的旋转操作（2*2旋转矩阵），每组的旋转角度计算如下：\n\n  \\theta_j = 10000^{-2(j-1)/d}, j \\in [1, 2, \\cdots, d/2]\n\n","categories":["AI","Embedding"]},{"title":"CNN","url":"/2025/09/24/NeuralNetwork/CNN/","content":"\n参考文章：https://zhuanlan.zhihu.com/p/494796637背景\n参数共享：图像需要处理的数据量太大，如果直接使用全连接层进行图像特征的提取，参数量会非常巨大，而且很容易过拟合。\n平移不变：假设图像中有一个圆形，如果采用传统的方法进行特征表示，圆形在左上角和在右下角得到的差异会非常大，而从视觉的角度来看，图像的内容并没有变化，只是其位置发生了变化。CNN 解决了这个问题，能够更加有效地保留图像的特征。\n人类视觉：CNN 与人类视觉的原理类似：从原始信号输入开始（像素），随后做初步处理（边缘、方向），然后进行抽象（形状），最后做出判断。CNN 类似于模仿人类大脑的这一过程，较低层识别初级图像特征，若干底层特征组成更上一层的特征，逐步向上最终做出分类。\n与传统的全连接网络不同的是，当前层的神经元只与上一层的部分神经元连接，连接通过卷积运算实现。\n\n模型结构\n\n输入层：三维矩阵的长和宽表示图像的大小，深度表示图像的色彩通道。\n卷积层：将图像的局部特征组合成抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵深度会增加，即通过学习不同的卷积核来组合成更多不同的高级特征。\n池化层：不会改变矩阵深度，只会减小矩阵大小，类似于降采样。主要用于减少最后全连接层中节点的个数，从而减少整个神经网络的参数。\n全连接层：经过卷积层和池化层的处理后，图像信息已经被抽象成信息密度更高的特征，卷积和池化的过程可以理解为自动提取图像特征的过程。全连接层通过对所有特征进行打分和组合，最终实现分类和回归任务。\nsoftmax层：用于分类问题，将全连接层的结果转换为概率。\nS_i = \\frac{e^i}{\\sum_j{e^j}}\n\n池化层的反向传播\n由于池化层没有参数参与正向传播过程，所以它是不可导的，无法直接进行反向传播。由于没有参数参与，因此一定要保证分配前后的梯度总和保持不变。\n常用的池化包括：\n最大池化：反向传播时将梯度直接传给最大值神经元\n平均池化：反向传播时将梯度平均分配给各个神经元\n\n\n\n","categories":["AI","神经网络"]},{"title":"RNN、LSTM和GRU","url":"/2025/09/24/NeuralNetwork/RNN%E3%80%81LSTM%E5%92%8CGRU/","content":"\n参考文章：\nhttps://zhuanlan.zhihu.com/p/123211148\nhttps://zhuanlan.zhihu.com/p/491564016\nhttps://zhuanlan.zhihu.com/p/32085405\nhttps://zhuanlan.zhihu.com/p/32481747\nhttps://zybuluo.com/hanbingtao/note/541458\nhttps://zybuluo.com/hanbingtao/note/581764RNN背景\n\n\n对于同一个单词，它在不同句子中的含义可能不同，传统的神经网络模型无法提取其在序列中的特征，预测的准确程度仅取决于训练集中label的数量。\n\n模型结构\n\n输入层：X 是一个向量，表示输入信息，与隐藏层之间不是全连接的，而是按照时刻与隐藏层对齐连接（例如输入一段英文，每个单词对应的向量与隐藏层对齐连接）\n隐藏层：S 是一个向量，表示隐藏层的值（节点数与 S 的维度相同）\n输出层：O 是一个向量，表示输出层的值\n模型参数：U 是输入层到隐藏层的权重矩阵，V 是隐藏层到输出层的权重矩阵，W 是隐藏层到隐藏层的权重矩阵，其中 V 和 W 都是全连接的，且各个时刻的 U、V 和 W 都共享各自的参数\n\n计算过程：\n\n  S_t = f(U \\cdot X_t + W \\cdot S_{t-1})\\\\\n  O_t = g(V \\cdot S_t)\n其中 f 和 g 为激活函数\n\n\n\n缺点\n由于整个计算过程都是顺序的，每次输入一个 embedding 顺序计算，后面得到的计算结果包含序列前面的 token 信息，但是前面得到的计算结果并不包含序列后面的 token 信息，因此表达能力有限。\n这种最基础的 RNN 存在梯度消失和梯度爆炸的问题，因此这种最朴素的 RNN 通常不会使用。\n\nRNN 梯度爆炸和梯度消失问题\n简单来说，RNN 梯度消失就是在反向传播过程中，梯度的计算中包含了矩阵的连乘，如果这些矩阵的特征值小于 1，则梯度会指数衰减，最终导致靠近输入层的梯度几乎为 0，参数得不到更新。梯度爆炸与梯度消失类似，如果这些矩阵的特征值大于 1，则梯度会指数增长，从而导致模型更新过大，无法收敛。\n具体解析如下：\n在 RNN 中，更新隐状态的矩阵 W_h 参与了所有时刻隐状态的更新，因此损失对其的梯度为：\n  \\frac{\\partial L}{\\partial W_h} = \\sum_{t = 1}^T\\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_h}\n其中 \\frac{\\partial h_t}{\\partial W_h} 可以直接求导得到。\n对于前项 \\frac{\\partial L}{\\partial h_t}，可以通过递推公式求得（默认激活函数为 tanh）：\n  \\frac{\\partial L}{\\partial h_t} = \\sum_{k = t}^T(\\frac{\\partial L}{\\partial h_k} \\prod_{j = t + 1}^k W_h^T \\cdot diag(1 - h_j^2))\n可以看到，这一部分的梯度包含了矩阵连乘，这也就是梯度爆炸和梯度消失的根源。\n该递推式可以如下理解：假设当前时刻为 t，那么 h_t 会参与 t 时刻之后的所有隐状态更新，因此 t 时刻的梯度需要对后续时刻的梯度求和得到。递推式的边界条件即为最后时刻的梯度，可以根据损失函数直接求导得到。\n\n\n\nLSTM\n为了解决 RNN 的梯度消失和梯度爆炸问题，提出了 LSTM（Long Short Term Memory Network）。\n原先的 RNN 隐藏层只有一个状态 h（hidden），其对于短期的输入非常敏感，因此 LSTM 的核心思路就是再添加一个隐藏层状态 c（cell），其在向前传递的过程中变化很慢，用于保存长期的状态。\nLSTM 引入了三个门控单元，用于控制 c 的状态与其对输出的贡献。遗忘门（forget gate）决定了上一时刻的 cell state c_{t-1} 有多少会保留到当前时刻 c_t；输入门（input gate）决定了当前时刻网络的输入 x_t 有多少会保存到 c_t；输出门（output gate）控制 c_t 有多少输出到 h_t。\n其具体结构与计算过程如下图所示：\n首先 LSTM 会先用当前输入 x^t 和上一个状态传递下来的 h^{t-1} 拼接并计算得到 4 个状态，其中 z 是输入，z^f, z^i, z^o 是三个门控单元，分别是遗忘门、输入门和输出门。\n  z = tanh(W \\cdot [x_t, h_{t-1}] + b) \\\\\n  z^f = sigmoid(W_f \\cdot [x_t, h_{t-1}] + b_f) \\\\\n  z^i = sigmoid(W_i \\cdot [x_t, h_{t-1}] + b_i) \\\\\n  z^o = sigmoid(W_o \\cdot [x_t, h_{t-1}] + b_o) \\\\\n如上图所示，LSTM 的内部计算主要包含三个阶段：\n忘记阶段：该阶段主要是对上一时刻的 cell 进行选择性遗忘，舍弃不重要信息，保留重要信息，这个过程由忘记门控 z^f 控制。\n选择记忆阶段：该阶段会对当前时刻输入 z 进行选择性记忆，这个过程由输入门控 z^i 控制。\n  c_t = z^f \\odot c_{t-1} + z^i \\odot z\n输出阶段：该阶段决定哪些 cell 的状态会作为当前状态输出，这个过程由输出门控 z^o 进行控制。\n  h_t = z^o \\odot tanh(c_t)\n\n\n最终网络的输出与 RNN 类似：\n  y_t = sigmoid(W^\\prime \\cdot h_t)\n\n为何 LSTM 能够解决 RNN 的梯度爆炸和梯度消失问题\nRNN 的梯度需要通过隐状态传递，从而出现了矩阵连乘形式，而 LSTM 通过引入 cell state 和遗忘门控使得梯度可以通过这条稳定的路径传递，只要有了这条梯度不会消失和爆炸的传递路径，我们就可以有效地更新模型的参数。\nLSTM 的 cell state 更新如下：\n  c_t = f_t \\odot c_{t-1} + i_t \\odot z\n对于 RNN 的隐状态更新，\\frac{\\partial h_t}{\\partial h_{t-1}} 可能与激活函数的导数相关，无法保证其稳定在 1 附近。\nLSTM 通过引入遗忘门控来保证 cell state 这条路径可以稳定传递梯度，即：\n  \\frac{\\partial c_t}{\\partial c_{t-1}} = f_t \\approx 1\n\n缺点\n相较于最朴素的 RNN，LSTM 引入了很多内容，导致参数变多，使得训练难度加大了很多，因此通常会使用效果和 LSTM 效果相当但参数更少的 GRU 来构建大模型。\n\nGRU\nGRU 的结构如图所示：\nGRU 引入了两个门控单元，其中 r 为重置门（reset gate），z 为更新门（update gate）：\n  r_t = sigmoid(W_r \\cdot [x_t, h_{t-1}])\n  z_t = sigmoid(W_z \\cdot [x_t, h_{t-1}])\n首先 GRU 会通过重置门将新的输入信息与前面的记忆相结合，具体公式如下：\n  h^\\prime = tanh(W \\cdot [x_t, r_t \\odot h_{t-1}])\nr_t 越小，则前面的记忆占比越小，表示上一时刻的记忆需要丢弃的越多；反之，r_t 越大，则说明上一时刻需要记住的越多。因此重置门是对前一时刻的记忆进行一定的重置，有助于提取序列中短期的关系。\n\n随后 GRU 会进行遗忘和选择记忆，即更新记忆，具体公式如下：\n\n  h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot h^\\prime\n(1 - z_t) \\odot h_{t-1} 表示对原本隐藏状态的选择性遗忘，z_t 越大，表示遗忘的更多，剔除记忆中不重要的信息。\nz_t \\odot h^\\prime表示对当前节点的信息 h^\\prime 进行选择性记忆，z_t 越大，表示保存到记忆中的信息越多。\n\n","categories":["AI","神经网络"]},{"title":"Transformer","url":"/2025/09/24/NeuralNetwork/Transformer/","content":"\n参考文章；\nhttps://blog.csdn.net/2401_85375298/article/details/144106338\nhttps://zybuluo.com/hanbingtao/note/2600518\nhttps://www.zybuluo.com/hanbingtao/note/2600833\n\n\n\nAttention\n注意力机制的核心，就是提升关键信息的权重，降低非关键信息的权重，通过给每个信息赋予不同的注意力权重，达到聚焦关键信息，忽略非关键信息的效果。\n\n举个例子，同样的单词在不同的上下文中，其含义是不一样的：\n\nThe animal didn’t cross the street because it was too tired.\nThe animal didn’t cross the street because it was too narrow.\n\n\n\nSelf Attention\n具体计算过程如下图所示：\n\nMulti-Head AttentionCross Attention ","categories":["AI","神经网络"]},{"title":"论文阅读：RenderFormer","url":"/2025/09/24/NeuralRendering/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARenderFormer/","content":"Introduction\n过去的神经渲染方法通常根据特定的场景对模型进行过拟合，本文提出了一整个神经渲染的pipeline，无需针对每个场景进行训练，只需要基于三角形mesh的场景描述作为输入，支持渲染全局光照。\n本文提出的方法将渲染公式转换为seq2seq变换，序列中的每个token都表示一个带有反射属性的三角形，随后被转换为具有光传输平衡（light transport equilibrium）的收敛辐射分布（converged radiance distribution）的三角形。\nRenderFormer主要包括两个阶段：\nview-independent阶段用于上述提到的反射属性三角形token到光传输三角形token的序列变换\nview-dependent阶段用于将上个阶段的三角形token转换为image\n两个阶段均使用transformer\n\n\n相较于传统的transformer位置编码，RenderFormer并没有选择使用1D序列索引编码，而是基于三角形的3D空间位置编码。\nRenderFormer完全基于可学习的神经组件，因此完全可微，不依赖于现有的固定渲染算法，例如光栅化、光线追踪或光线行进（ray marching）。\n先前的神经渲染方法使用的神经场景表示需要特殊的方法来修改场景，而RenderFormer采用基于三角形mesh的场景描述作为输入，能够兼容现有的场景编辑tool-chain。\n\nChallenge\n由于transformer的计算开销，目前RenderFormer限制三角形mesh的最大数量为4096。\n目前RenderFormer受限于训练过程中看到的变化：目前训练数据只包含了一种反射率模型，其参数是基于每个三角形的（即没有纹理）。\n目前训练的场景最多只包含8个漫射光源，相机分辨率固定为512 x 512，放置在场景包围盒外。\n\nView-independent StageTransformer Architecture\n该阶段的采用的transformer与原始的transformer类似，采用full bidirectional self-attention，输入序列由三角形token组成，每个三角形token编码了所有渲染相关的信息，例如表面法向量和反射率等。\n除此之外，模型还添加了16个register token，用于存储全局信息，并且有可能消除embedding中的高频噪声。\n每个三角形token和register token都是768维的向量。本阶段由12个transform层构成，每层包含6个head和768个hidden unit，然后是一个768 x 4的前馈全连接层。\n采用LLaMA，使用RMS-Normalization进行预归一化，使用SwiGLU作为激活函数。此外，还使用了QK-Normalization来稳定训练。\n\nRelative Spatial Positional Embedding\nRenderFormer之所以不与传统的transformer一样使用索引位置编码，是因为三角形的索引位置信息在序列中是不相关的（交换序列中的两个三角形顺序，最终产生的结果是相同的）。然而，三角形本身的位置信息是相关的，即使两个三角形拥有相同形状和反射性质，如果位置不同，其对global light transport的贡献也是不同的。\n与此同时，对整个场景进行平移也不会改变light transport。因此，RenderFormer需要的是基于每个三角形相对于其他三角形的3D空间位置的相对位置编码。RenderFormer采用RoPE来嵌入相对空间位置。\n然而，RenderFormer没有使用序列索引计算位置编码，而是使用三角形的3D顶点空间信息（浮点数），因此还需要进行一系列处理才能进行RoPE编码：\n首先将三个顶点的三维坐标顺序拼接成一个 9D 向量\n随后将每个分量复制 6 份，这 6 份分量分别乘上相应的频率信息（[1.0, 1.3797, 1.9037, 2.6265, 3.6239, 5.0]），得到一个包含不同频率空间特征的 54 维向量。\n为了使用 RoPE 进行位置编码，还需要对这 54 维向量的每一个分量求 sin 与 cos，即 54 个旋转矩阵，从而能够对 embedding 的前 108 维进行 RoPE 编码。\n\n\nRenderFormer 的 Transformer 每个注意力头处理 128 维的 embedding，因此只对前 108 维进行 RoPE 编码，剩下 20 维分量保持不变。（论文使用的 Transformer 共有 6 个注意力头，可以处理 768 维的三角形 token embedding）\n上文提到的相对位置编码与 RoPE 类似，都能够在保留原有内容的前提下，通过二维旋转引入相对位置信息。\n三维 RoPE 无法保证旋转不变性：\n在 2D 空间中，用 RoPE 进行位置编码，注意力分数的位置信息只依赖于旋转角度差 \\theta_j - \\theta_i，不管绝对位置是多少，注意力分数只和相对位置有关。这种旋转不变性保证了相对位置在不同全局坐标系下能够保持一致。\n而在 3D 空间中，一个旋转不是单靠一个旋转角度就能确定的，即便是相同的角度，更换旋转轴或旋转顺序，最终得到的旋转结果完全不同（SO(3) 不可交换），此时无法通过角度差来得到相对位置，因此无法对相对位置做统一表达。\n\n\n\nTriangle Embedding","categories":["NeuralRendering"]},{"title":"RegisterToken","url":"/2025/09/24/Token/RegisterToken/","content":"（待完善）\n问题：\n大的模型或经过长时间的迭代，会让模型关注到一些冗余的 token，这些token中原有的局部特征会被替换为保存全局信息的特征。这些token本质上并没有问题，它们出现在冗余部分，也可以靠周围的token来补充局部信息，但是会使模型在特定任务上效果变差。解决方法：\n新增一些register tokens，让网络中的这些register token去学习全局特征，而不要让应该学习局部特征的token变成拥有全局信息的artifact token，训练完成后再将register tokens丢弃掉。\n\n","categories":["AI","Token"]}]