[{"title":"FourierFeatureMapping","url":"/2025/09/24/Embedding/FourierFeatureMapping/","content":"（待完善）\n在生成 Embedding 时的应用\n例如使用三角形 mesh 顶点的三维空间信息进行位置编码时，可以将三个顶点的三维坐标先拼接为 9D 向量，随后将每个分量复制 6 份，这 6 份分量分别乘上相应的频率信息（[1.0, 1.3797, 1.9037, 2.6265, 3.6239, 5.0]），得到一个包含不同频率空间特征的 54 维向量。\n如果需要学习相对位置信息，则可以对这 54 维向量的每一个分量求 sin 与 cos，即 54 个旋转矩阵，从而可以用于RoPE编码（覆盖 embedding 的前 108 维）\n如何理解学习不同频率的空间特征：\n小频率可以表示大尺度的特征，如三角形的全局位置\n大频率可以表示小尺度的特征，如高频细节、边界、形状等\n通过将原始数据乘上不同频率值，可以提取到更多特征信息，从而使模型能够表达更加复杂的函数。\n\n\n\n","categories":["AI","Embedding"]},{"title":"CNN","url":"/2025/09/24/NeuralNetwork/CNN/","content":"\n参考文章：https://zhuanlan.zhihu.com/p/494796637背景\n参数共享：图像需要处理的数据量太大，如果直接使用全连接层进行图像特征的提取，参数量会非常巨大，而且很容易过拟合。\n平移不变：假设图像中有一个圆形，如果采用传统的方法进行特征表示，圆形在左上角和在右下角得到的差异会非常大，而从视觉的角度来看，图像的内容并没有变化，只是其位置发生了变化。CNN 解决了这个问题，能够更加有效地保留图像的特征。\n人类视觉：CNN 与人类视觉的原理类似：从原始信号输入开始（像素），随后做初步处理（边缘、方向），然后进行抽象（形状），最后做出判断。CNN 类似于模仿人类大脑的这一过程，较低层识别初级图像特征，若干底层特征组成更上一层的特征，逐步向上最终做出分类。\n与传统的全连接网络不同的是，当前层的神经元只与上一层的部分神经元连接，连接通过卷积运算实现。\n\n模型结构\n\n输入层：三维矩阵的长和宽表示图像的大小，深度表示图像的色彩通道。\n卷积层：将图像的局部特征组合成抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵深度会增加，即通过学习不同的卷积核来组合成更多不同的高级特征。\n池化层：不会改变矩阵深度，只会减小矩阵大小，类似于降采样。主要用于减少最后全连接层中节点的个数，从而减少整个神经网络的参数。\n全连接层：经过卷积层和池化层的处理后，图像信息已经被抽象成信息密度更高的特征，卷积和池化的过程可以理解为自动提取图像特征的过程。全连接层通过对所有特征进行打分和组合，最终实现分类和回归任务。\nsoftmax层：用于分类问题，将全连接层的结果转换为概率。\nS_i = \\frac{e^i}{\\sum_j{e^j}}\n\n池化层的反向传播\n由于池化层没有参数参与正向传播过程，所以它是不可导的，无法直接进行反向传播。由于没有参数参与，因此一定要保证分配前后的梯度总和保持不变。\n常用的池化包括：\n最大池化：反向传播时将梯度直接传给最大值神经元\n平均池化：反向传播时将梯度平均分配给各个神经元\n\n\n\n","categories":["AI","神经网络模型"],"tags":["CNN"]},{"title":"RNN、LSTM和GRU","url":"/2025/09/24/NeuralNetwork/RNN%E3%80%81LSTM%E5%92%8CGRU/","content":"\n参考文章：\nhttps://zhuanlan.zhihu.com/p/123211148\nhttps://zhuanlan.zhihu.com/p/491564016\nhttps://zhuanlan.zhihu.com/p/32085405\nhttps://zhuanlan.zhihu.com/p/32481747\nhttps://zybuluo.com/hanbingtao/note/541458\nhttps://zybuluo.com/hanbingtao/note/581764RNN背景\n\n\n对于同一个单词，它在不同句子中的含义可能不同，传统的神经网络模型无法提取其在序列中的特征，预测的准确程度仅取决于训练集中label的数量。\n\n模型结构\n\n输入层：X 是一个向量，表示输入信息，与隐藏层之间不是全连接的，而是按照时刻与隐藏层对齐连接（例如输入一段英文，每个单词对应的向量与隐藏层对齐连接）\n隐藏层：S 是一个向量，表示隐藏层的值（节点数与 S 的维度相同）\n输出层：O 是一个向量，表示输出层的值\n模型参数：U 是输入层到隐藏层的权重矩阵，V 是隐藏层到输出层的权重矩阵，W 是隐藏层到隐藏层的权重矩阵，其中 V 和 W 都是全连接的，且各个时刻的 U、V 和 W 都共享各自的参数\n\n计算过程：\n\n  S_t = f(U \\cdot X_t + W \\cdot S_{t-1})\\\\\n  O_t = g(V \\cdot S_t)\n其中 f 和 g 为激活函数\n\n\n\n缺点\n由于整个计算过程都是顺序的，每次输入一个 embedding 顺序计算，后面得到的计算结果包含序列前面的 token 信息，但是前面得到的计算结果并不包含序列后面的 token 信息，因此表达能力有限。\n这种最基础的 RNN 存在梯度消失和梯度爆炸的问题，因此这种最朴素的 RNN 通常不会使用。\n\nRNN 梯度爆炸和梯度消失问题\n简单来说，RNN 梯度消失就是在反向传播过程中，梯度的计算中包含了矩阵的连乘，如果这些矩阵的特征值小于 1，则梯度会指数衰减，最终导致靠近输入层的梯度几乎为 0，参数得不到更新。梯度爆炸与梯度消失类似，如果这些矩阵的特征值大于 1，则梯度会指数增长，从而导致模型更新过大，无法收敛。\n具体解析如下：\n在 RNN 中，更新隐状态的矩阵 W_h 参与了所有时刻隐状态的更新，因此损失对其的梯度为：\n  \\frac{\\partial L}{\\partial W_h} = \\sum_{t = 1}^T\\frac{\\partial L}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_h}\n其中 \\frac{\\partial h_t}{\\partial W_h} 可以直接求导得到。\n对于前项 \\frac{\\partial L}{\\partial h_t}，可以通过递推公式求得（默认激活函数为 tanh）：\n  \\frac{\\partial L}{\\partial h_t} = \\sum_{k = t}^T(\\frac{\\partial L}{\\partial h_k} \\prod_{j = t + 1}^k W_h^T \\cdot diag(1 - h_j^2))\n可以看到，这一部分的梯度包含了矩阵连乘，这也就是梯度爆炸和梯度消失的根源。\n该递推式可以如下理解：假设当前时刻为 t，那么 h_t 会参与 t 时刻之后的所有隐状态更新，因此 t 时刻的梯度需要对后续时刻的梯度求和得到。递推式的边界条件即为最后时刻的梯度，可以根据损失函数直接求导得到。\n\n\n\nLSTM\n为了解决 RNN 的梯度消失和梯度爆炸问题，提出了 LSTM（Long Short Term Memory Network）。\n原先的 RNN 隐藏层只有一个状态 h（hidden），其对于短期的输入非常敏感，因此 LSTM 的核心思路就是再添加一个隐藏层状态 c（cell），其在向前传递的过程中变化很慢，用于保存长期的状态。\nLSTM 引入了三个门控单元，用于控制 c 的状态与其对输出的贡献。遗忘门（forget gate）决定了上一时刻的 cell state c_{t-1} 有多少会保留到当前时刻 c_t；输入门（input gate）决定了当前时刻网络的输入 x_t 有多少会保存到 c_t；输出门（output gate）控制 c_t 有多少输出到 h_t。\n其具体结构与计算过程如下图所示：\n首先 LSTM 会先用当前输入 x^t 和上一个状态传递下来的 h^{t-1} 拼接并计算得到 4 个状态，其中 z 是输入，z^f, z^i, z^o 是三个门控单元，分别是遗忘门、输入门和输出门。\n  z = tanh(W \\cdot [x_t, h_{t-1}] + b) \\\\\n  z^f = sigmoid(W_f \\cdot [x_t, h_{t-1}] + b_f) \\\\\n  z^i = sigmoid(W_i \\cdot [x_t, h_{t-1}] + b_i) \\\\\n  z^o = sigmoid(W_o \\cdot [x_t, h_{t-1}] + b_o) \\\\\n如上图所示，LSTM 的内部计算主要包含三个阶段：\n忘记阶段：该阶段主要是对上一时刻的 cell 进行选择性遗忘，舍弃不重要信息，保留重要信息，这个过程由忘记门控 z^f 控制。\n选择记忆阶段：该阶段会对当前时刻输入 z 进行选择性记忆，这个过程由输入门控 z^i 控制。\n  c_t = z^f \\odot c_{t-1} + z^i \\odot z\n输出阶段：该阶段决定哪些 cell 的状态会作为当前状态输出，这个过程由输出门控 z^o 进行控制。\n  h_t = z^o \\odot tanh(c_t)\n\n\n最终网络的输出与 RNN 类似：\n  y_t = sigmoid(W^\\prime \\cdot h_t)\n\n为何 LSTM 能够解决 RNN 的梯度爆炸和梯度消失问题\nRNN 的梯度需要通过隐状态传递，从而出现了矩阵连乘形式，而 LSTM 通过引入 cell state 和遗忘门控使得梯度可以通过这条稳定的路径传递，只要有了这条梯度不会消失和爆炸的传递路径，我们就可以有效地更新模型的参数。\nLSTM 的 cell state 更新如下：\n  c_t = f_t \\odot c_{t-1} + i_t \\odot z\n对于 RNN 的隐状态更新，\\frac{\\partial h_t}{\\partial h_{t-1}} 可能与激活函数的导数相关，无法保证其稳定在 1 附近。\nLSTM 通过引入遗忘门控来保证 cell state 这条路径可以稳定传递梯度，即：\n  \\frac{\\partial c_t}{\\partial c_{t-1}} = f_t \\approx 1\n\n缺点\n相较于最朴素的 RNN，LSTM 引入了很多内容，导致参数变多，使得训练难度加大了很多，因此通常会使用效果和 LSTM 效果相当但参数更少的 GRU 来构建大模型。\n\nGRU\nGRU 的结构如图所示：\nGRU 引入了两个门控单元，其中 r 为重置门（reset gate），z 为更新门（update gate）：\n  r_t = sigmoid(W_r \\cdot [x_t, h_{t-1}])\n  z_t = sigmoid(W_z \\cdot [x_t, h_{t-1}])\n首先 GRU 会通过重置门将新的输入信息与前面的记忆相结合，具体公式如下：\n  h^\\prime = tanh(W \\cdot [x_t, r_t \\odot h_{t-1}])\nr_t 越小，则前面的记忆占比越小，表示上一时刻的记忆需要丢弃的越多；反之，r_t 越大，则说明上一时刻需要记住的越多。因此重置门是对前一时刻的记忆进行一定的重置，有助于提取序列中短期的关系。\n\n随后 GRU 会进行遗忘和选择记忆，即更新记忆，具体公式如下：\n\n  h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot h^\\prime\n(1 - z_t) \\odot h_{t-1} 表示对原本隐藏状态的选择性遗忘，z_t 越大，表示遗忘的更多，剔除记忆中不重要的信息。\nz_t \\odot h^\\prime表示对当前节点的信息 h^\\prime 进行选择性记忆，z_t 越大，表示保存到记忆中的信息越多。\n\n","categories":["AI","神经网络模型"],"tags":["RNN","LSTM","GRU"]},{"title":"Transformer","url":"/2025/09/24/NeuralNetwork/Transformer/","content":"\n参考文章；\nhttps://blog.csdn.net/2401_85375298/article/details/144106338\nhttps://zybuluo.com/hanbingtao/note/2600518\nhttps://www.zybuluo.com/hanbingtao/note/2600833\nhttps://blog.csdn.net/shizheng_Li/article/details/146213459\nhttps://zhuanlan.zhihu.com/p/693611439\n\n\n\nAttention\n注意力机制的核心，就是提升关键信息的权重，降低非关键信息的权重，通过给每个信息赋予不同的注意力权重，达到聚焦关键信息，忽略非关键信息的效果。\n\n举个例子，同样的单词在不同的上下文中，其含义是不一样的：\n\nThe animal didn’t cross the street because it was too tired.\nThe animal didn’t cross the street because it was too narrow.\n\n\n我们希望每个 token 能够根据上下文的不同，分配不同的注意力权重，即 Self Attention。其核心是序列中的任意两个 token 之间都计算注意力权重，也就是每个 token 都要注意自身及序列中的其他 token，因此叫做自注意力机制。\n\nSelf Attention\n具体计算过程如下图所示：\n\n第一步，序列在输入之前，首先要将 token 转换为向量，按行拼接成矩阵 X。随后，token 向量矩阵分别右乘三个可学习的参数矩阵 W_Q、W_K、W_V，得到 Q（Query）、K（Key）、V（Value）矩阵，其中 Q 和 K 中的向量维度是相同的，而 V 的维度可以与 Q、K 不同。\n\n第二步，计算 token 之间的注意力权重，具体做法就是将自身的 Q 矩阵与序列中其他所有 token 的 K 矩阵相乘，即为当前 token 对序列中其他 token 的注意力。若向量的维度较大，则最终的点乘结果也会显著增大，从而导致梯度消失，因此还需要对点乘结果进行一定的缩放，通常采用的系数是 \\frac{1}{\\sqrt{D_k}}。\n第三步，对缩放后的注意力分数进行归一化，通常采用 softmax 函数进行归一化，保证最终该 token 注意力分数之和为 1，相当于转换成概率。\n最后，将归一化后的注意力分数与其对应的 token 相乘并求和，最终得到包含自身与上下文的向量表示。\n整个过程可以这样理解：Q 矩阵是提问者，包含想要关注什么的向量，K 矩阵是被关注者的索引或标识，通过 Q 与 K 相乘，就可以得到各个 token 之间的关注度。V 矩阵是被关注者实际携带的信息，我们根据先前计算得到的关注度进行加权求和，最终得到的结果就包含了自身和相关上下文（Value）的信息。\n\nMulti-Head Attention\n\n如果仅仅为一个 token 线性投影出一份 Q、K、V 矩阵，那多个语义子空间的信息就被平均化了，模型就很难关注来自不同语义子空间的信息。以看电影为例，如果是 Self Attention，我们只能整体评价一部电影好还是不好，但如果采用 Multi-Head Attention，那我们就可以从不同的方面，如音乐、画面等，对电影进行评价。\n\n具体做法就是每个 token 线性投影得到 h 个 Q、K、V 矩阵，每一组都进行 Self Attention，得到的注意力分数矩阵 Z 拼接在一起，最终与一个可学习的权重矩阵相乘，统合各个语义子空间的信息，使最终的结果维度与输入的词向量相同。\n\n\nMasked Multi-Head Attention\n对于文本理解，注意力机制的设计为双向的（即前文提到的各种注意力机制），一个 token 既可以注意到前面的 token，也可以注意到后续的 token。\n对于文本生成，注意力机制的设计是单向的（当前 token 无法注意到后续尚未生成的 token）。单向注意力机制可以通过掩码实现，定义掩码矩阵 M \\in R^{S \\times S}：\nM_{ij} = \\begin{cases}\n0, & i \\geq j \\\\\n-\\infty, & i < j\n\\end{cases}\n其中，i 表示 query 向量在序列中的位置，j 表示 key 向量在序列中的位置，该掩码矩阵本质上是一个上三角矩阵。\n在计算注意力权重时，掩码是通过加法引入注意力分数中：\nMasked(A) = A + M\n因此该注意力公式为：\nCausalAttention(Q, K, V) = softmax\\left( \\frac{QK^T}{\\sqrt{d_k}} + M \\right) V\n当掩码为 0 时，注意力分数和 Self Attention 计算方法一致；当掩码为 -\\infty时，经过 softmax 计算，最终注意力分数趋向于 0，忽略不计。\n简单来说，掩码通过将当前 token 对后续 token 的注意力分数强制清 0，最终计算结果不包含任何后文 token 的 Value 信息，从而达到单向注意力的效果。\n\nCross Attention\n\n与 Self Attention 不同，Self Attention 是让一个序列内部的元素相互关注，而 Cross Attention 则是让两个不同的序列之间建立关注关系，其核心在于允许一个序列（query）去关注另一个序列（key、value），从而实现信息的融合。\n具体来说，Query 的来源通常是一个需要补充信息的目标序列，Key/Value 的来源通常是一个提供信息的参考序列。整个过程就可以理解为：一个需要生成或理解的目标序列（Query）去关注一个提供上下文或背景的参考序列（Key/Value），最终根据计算的关注度和参考序列信息来补充生成目标序列的后续内容。\n举几个例子：\n\n机器翻译（seq2seq）\nQuery：Decoder 当前生成的单词\nKey/Value：Encoder 输出的源语言句子\n交叉关系：Decoder 在生成目标语言时，关注源语言的每个单词，决定当前要翻译什么\n\n\n图像描述生成\nQuery：语言模型生成的当前单词\nKey/Value：图像特征（由 CNN 或 Transformer 提取）\n交叉关系：语言模型在生成描述时，关注源图像的不同特征\n\n\n多模态任务\nQuery：文本输入（例如问题）\nKey/Value：图像或视频特征\n交叉关系：文本去关注与问题相关的视觉信息，从而完成回答\n\n\n\n\nCross Attention 本质是将两种模态的信息投影到同一个局部语义交互空间，在该空间中可以让不同模态的信息进行语义指导与语义交互；而 CLIP 的文本与图像投影到的是同一个全局语义对齐空间，比前者更加严格，能够做到图文的一一匹配。\n\n待生成的一方通常提供 Query 向量，而控制信息/参考信息通常提供 Key 和 Value 向量。投影到同一个语义空间后，通过注意力机制即可实现语义交互，待生成的一方通过 Query 向量与参考信息的 Key 进行点积，使得相关度更高的参考信息权重更高，加权求和后，最终结果就包含了参考信息，从而实现跨模态的引导生成。\n\nTransformer 结构\nTransformer 整体上采用的是 Encoder-Decoder 结构，如图所示：\n\nEncoder、Decoder\n对于 Encoder-Decoder 模型来说，Encoder 通常是用来理解输入，而 Decoder 用于生成输出，主要包括以下三类模型：\nEncoder only：主要用于判别类任务，如分类、回归。\nDecoder only：可以用于生成序列，需要大量训练语料，没办法直接高效处理输入-输出匹配任务。以 GPT 为例，GPT 与用户的交互并不是输入-输出匹配任务，而是采取 Decoder 堆叠，在训练时，模型通过大量训练数据学会了根据上下文预测合理的后续内容，在推理时，用户输入一个 prompt，模型会把这个 prompt 当做上下文，然后继续往后生成回答。\nEncoder-Decoder：最早主要用于 seq2seq 任务，Encoder 用于理解输入，将输入的序列转到中间语义空间，Decoder 再从中间语义空间解码，生成输出结果。\n\n\n\nTransformer Encoder 结构\n首先，需要将输入序列中的 tokens 转换为词向量，并引入位置编码，得到的 embeddings 将作为 Encoder 的起始输入。\nEncoder 的内部结构主要由 Multi-Head Attention、Add &amp; Norm、Feed Forward 等构成：\nMulti-Head Attention：主要用于获取序列中 token 之间的上下文信息。\nFeed Forward：主要用于每个 token 内部各个维度的特征信息的混合，相当于按照一定的权重组合各个原始特征，得到表达能力更强的高阶特征，进一步增强模型的表达能力，具体公式为：\nFFN(x) = max(0, xW_1 + b_1)W_2 + b_2\nAdd &amp; Norm：\nAdd 操作本质是残差网络，保证梯度反向传播时可以跳过 Multi-Head Attention 和 FFN 等子层，避免梯度爆炸和梯度消失，同时也可以通过 引入恒等变换进一步增强模型的表达能力。\nNorm 是归一化，如果不进行归一化，某一层的输出经过激活函数后，其分布可能会出现较大的偏移，例如输出值变得特别大，那么在梯度反向传播时就有可能出现梯度爆炸。与此同时，若不同层的输出分布差异很大，例如有的层输出分布很集中，而有的层输出分布很分散，在训练过程中很难找到一个合适的学习率去适应不同的尺度。因此归一化对模型的训练收敛是不可或缺的，能够保证训练稳定性的同时，加速模型收敛。\nTransformer 中常用的归一化公式如下，其中 \\mu 是单个 token 所有维度的均值，\\sigma 是单个 token 所有维度的标准差，\\gamma、\\beta 是模型的可学习参数，对归一化后的 token 的每个维度进行适当的仿射变换，使归一化后的数据分布更符合激活函数的非线性变化。\nLayerNorm(x) = \\frac{x - \\mu}{\\sigma} * \\gamma + \\beta\n\n\n\n\n上面所介绍的 Encoder 结构属于 Post-Norm 型，是 Transformer 提出时所采用的结构，即 x → Attention/FFN → 残差加和 → LayerNorm，这种结构梯度在反向传播时必须先经过 LayerNorm 才能继续传递，当网络很深（上百层）时，还是容易出现梯度爆炸和梯度消失。\n目前更常用的 Encoder 结构是 Pre-Norm 型，每一个 Encoder 单元在输入信息后先进行归一化，即 x → LayerNorm → Attention/FFN → 残差求和，这样梯度在反向传播时可以跳过 LayerNorm 层。这种结构不需要保证单元输出为归一化后的稳定分布，而是在输入时先进行归一化，同样可以得到 Post-Norm 结构的效果，并且训练更加稳定，支持更深层的网络结构。\n\nTransformer Decoder 结构\n\nDecoder 的输入输出序列中包含两个特殊的 token：[start] 是解码器输入的第一个 token，用于标识 decoding 开始，随后解码器开始生成新序列的第一个 token；[end] 是解码器输出的最后一个 token，用于标识生成结束。\nDecoder 的结构与 Encoder 基本相似，有两点不同：\n第一点不同，Decoder 只能根据已经生成的 token 序列去生成后续的 token，因此需要使用 Masked Multi-Head Attention 替代原先的 Multi-Head Attention。\n第二点不同，Decoder 在生成过程中，不仅需要结合已经生成的上下文信息，还要结合 Encoder 提供的输入信息，因此还需要一个额外的 Multi-Head Attention 层，用于实现 Encoder 和 Decoder 的 Cross Attention。\n具体来说，将 Decoder 中 Masked Multi-Head Attention 层的输出投影得到 query 向量，根据 Encoder 对应层的输出投影得到 key 和 value 向量，再根据注意力公式进行计算，从而使得 Decode 输出的 token 可以融合 Encoder 输出 token 的信息。\n\n\n简单来说，第一层 Masked Multi-Head Attention 用于目标序列关注自身（已生成的 token 之间相互关注），第二层 Cross Attention 用目标序列的 Query 去关注 Encoder 输出的参考信息（Key/Value）。\n当 Decoder 的最后一个单元输出后，这个输出的矩阵每一行都代表了该 token 经过注意力机制计算后所包含的信息（已生成目标序列的上下文信息、参考序列信息），随后这个矩阵会经过一次线性变换，将维度从词向量维度 D_x 变为词表的大小，再经过带温度的 softmax 归一化后，使之转变为概率，归一化后的向量的每个分量都对应词表中的词的预测概率，对这个概率进行采样，就可以输出对应的词。\n带温度的 softmax 公式如下：\nSoftmax(x_i) = \\frac{e^{x_i/T}}{\\sum_{j=1}^n e^{x_j/T}}\n其中 x_i 为线性变换后的 Decoder 输出向量分量，T 是温度参数。\n当温度 T 趋近于 0 时，概率分布会变得非常尖锐，模型的输出确定性加强，当 T = 0 时，模型只会输出最高值对应的词；当 T 的取值增大后，概率分布会变得平滑，模型输出的随机性加强，会表现出更多的创造力。\n\nTransformer 训练过程：\n\nEncoder 阶段的过程与先前介绍的 Encoder 结构一致，此处不再介绍。\n需要注意的是，Decoder 阶段的输入是一整个目标序列，因为在 Decoding 的过程中，Masked Multi-Head Attention 模拟了生成过程中的单向注意力，因此不需要像推理生成那样循环执行，从而加快训练速度。\n为了避免混淆，这里引入了掩码之后，最后 Decoder 输出的矩阵每一行并不是代表每个时刻的序列信息，其仍然是每个 token 经过注意力机制计算后得到的融合信息，只是该 token 所对应的行向量不包含其后续 token 的 Value 信息，因此掩码可以有效地模拟整个循环生成的过程。\n得到概率之后，可以通过采样或取概率最高的词作为生成结果，训练时直接得到整个生成的序列，再根据损失函数继续对模型进行参数优化即可。\n\nTransformer 推理过程：\n\n与训练过程不同的是，模型不知道目标序列是什么，因此只能逐个单词进行生成。\n编码阶段与训练过程完全一致，解码阶段初始并不知道任何目标序列的信息，因此只能从起始标志 [start] 开始输入 Decoder，经过一轮解码后，根据概率采样或选取最大值对应的词，将其拼接到正在生成的目标序列当中，然后将拼接好的目标序列再次输入到 Decoder 中生成下一个单词，直至整个序列生成完毕。\n\n","categories":["AI","神经网络模型"],"tags":["Transformer"]},{"title":"论文阅读：RenderFormer","url":"/2025/09/24/NeuralRendering/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARenderFormer/","content":"（待完善）\nIntroduction\n过去的神经渲染方法通常根据特定的场景对模型进行过拟合，本文提出了一整个神经渲染的pipeline，无需针对每个场景进行训练，只需要基于三角形mesh的场景描述作为输入，支持渲染全局光照。\n本文提出的方法将渲染公式转换为seq2seq变换，序列中的每个token都表示一个带有反射属性的三角形，随后被转换为具有光传输平衡（light transport equilibrium）的收敛辐射分布（converged radiance distribution）的三角形。\nRenderFormer主要包括两个阶段：\nview-independent阶段用于上述提到的反射属性三角形token到光传输三角形token的序列变换\nview-dependent阶段用于将上个阶段的三角形token转换为image\n两个阶段均使用transformer\n\n\n相较于传统的transformer位置编码，RenderFormer并没有选择使用1D序列索引编码，而是基于三角形的3D空间位置编码。\nRenderFormer完全基于可学习的神经组件，因此完全可微，不依赖于现有的固定渲染算法，例如光栅化、光线追踪或光线行进（ray marching）。\n先前的神经渲染方法使用的神经场景表示需要特殊的方法来修改场景，而RenderFormer采用基于三角形mesh的场景描述作为输入，能够兼容现有的场景编辑tool-chain。\n\nChallenge\n由于transformer的计算开销，目前RenderFormer限制三角形mesh的最大数量为4096。\n目前RenderFormer受限于训练过程中看到的变化：目前训练数据只包含了一种反射率模型，其参数是基于每个三角形的（即没有纹理）。\n目前训练的场景最多只包含8个漫射光源，相机分辨率固定为512 x 512，放置在场景包围盒外。\n\nView-independent StageTransformer Architecture\n该阶段的采用的transformer与原始的transformer类似，采用full bidirectional self-attention，输入序列由三角形token组成，每个三角形token编码了所有渲染相关的信息，例如表面法向量和反射率等。\n除此之外，模型还添加了16个register token，用于存储全局信息，并且有可能消除embedding中的高频噪声。\n每个三角形token和register token都是768维的向量。本阶段由12个transform层构成，每层包含6个head和768个hidden unit，然后是一个768 x 4的前馈全连接层。\n采用LLaMA，使用RMS-Normalization进行预归一化，使用SwiGLU作为激活函数。此外，还使用了QK-Normalization来稳定训练。\n\nRelative Spatial Positional Embedding\nRenderFormer之所以不与传统的transformer一样使用索引位置编码，是因为三角形的索引位置信息在序列中是不相关的（交换序列中的两个三角形顺序，最终产生的结果是相同的）。然而，三角形本身的位置信息是相关的，即使两个三角形拥有相同形状和反射性质，如果位置不同，其对global light transport的贡献也是不同的。\n与此同时，对整个场景进行平移也不会改变light transport。因此，RenderFormer需要的是基于每个三角形相对于其他三角形的3D空间位置的相对位置编码。RenderFormer采用RoPE来嵌入相对空间位置。\n然而，RenderFormer没有使用序列索引计算位置编码，而是使用三角形的3D顶点空间信息（浮点数），因此还需要进行一系列处理才能进行RoPE编码：\n首先将三个顶点的三维坐标顺序拼接成一个 9D 向量\n随后将每个分量复制 6 份，这 6 份分量分别乘上相应的频率信息（[1.0, 1.3797, 1.9037, 2.6265, 3.6239, 5.0]），得到一个包含不同频率空间特征的 54 维向量。\n为了使用 RoPE 进行位置编码，还需要对这 54 维向量的每一个分量求 sin 与 cos，即 54 个旋转矩阵，从而能够对 embedding 的前 108 维进行 RoPE 编码。\n\n\nRenderFormer 的 Transformer 每个注意力头处理 128 维的 embedding，因此只对前 108 维进行 RoPE 编码，剩下 20 维分量保持不变。（论文使用的 Transformer 共有 6 个注意力头，可以处理 768 维的三角形 token embedding）\n上文提到的相对位置编码与 RoPE 类似，都能够在保留原有内容的前提下，通过二维旋转引入相对位置信息。\n三维 RoPE 无法保证旋转不变性：\n在 2D 空间中，用 RoPE 进行位置编码，注意力分数的位置信息只依赖于旋转角度差 \\theta_j - \\theta_i，不管绝对位置是多少，注意力分数只和相对位置有关。这种旋转不变性保证了相对位置在不同全局坐标系下能够保持一致。\n而在 3D 空间中，一个旋转不是单靠一个旋转角度就能确定的，即便是相同的角度，更换旋转轴或旋转顺序，最终得到的旋转结果完全不同（SO(3) 不可交换），此时无法通过角度差来得到相对位置，因此无法对相对位置做统一表达。\n\n\n\nTriangle Embedding","categories":["NeuralRendering"],"tags":["Transformer","NeuralRendering"]},{"title":"RegisterToken","url":"/2025/09/24/Token/RegisterToken/","content":"（待完善）\n问题：\n大的模型或经过长时间的迭代，会让模型关注到一些冗余的 token，这些token中原有的局部特征会被替换为保存全局信息的特征。这些token本质上并没有问题，它们出现在冗余部分，也可以靠周围的token来补充局部信息，但是会使模型在特定任务上效果变差。解决方法：\n新增一些register tokens，让网络中的这些register token去学习全局特征，而不要让应该学习局部特征的token变成拥有全局信息的artifact token，训练完成后再将register tokens丢弃掉。\n\n","categories":["AI","Token"]},{"title":"ResNet","url":"/2025/09/25/NeuralNetwork/ResNet/","content":"\n参考文章：\nhttps://zh.d2l.ai/chapter_convolutional-modern/resnet.html\n\n\n\n背景\n理论上，增加神经网络的层数，能够得到表达能力更强的模型，至少不会比浅层神经网络更差，但实际上，深层网络得到的训练误差因为优化失败反而更高，这也意味着深层网络连”什么都不做“的恒等变换都无法学会。残差网络的目标之一就是：至少要保证加深网络层数不会使得性能退化，即学会恒等映射。\n另一方面，随着深度神经网络的深度不断增加，梯度传递也越来越困难，出现梯度消失或梯度爆炸的问题，训练难以收敛。\n\n设计思路\n\n网络很难直接学习到恒等映射 F(x) = x，而如果把目标函数改为 H(x) = F(x) + x，要想得到恒等映射 H(x) = x，那网络只需要拟合残差映射 F(x) = H(x) - x 即可，换句话说，网络不需要直接学习完整的变化，而是学输入与输出之间的差值，如果最优解真的是 H(x) = x，那么网络只需要学到 F(x) = 0。\n另一方面，这种残差设计更有利于梯度传递，在反向传播时，梯度按如下公式进行传播：\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H(x)} \\cdot \\left( 1 + \\frac{\\partial F(x)}{\\partial x} \\right)\n当 \\frac{\\partial F(x)}{\\partial x} 很小时，如果不使用残差网络，深度增加可能会导致梯度消失，底层参数得不到更新。而在残差网络中，即使该项很小，梯度仍可以通过“+1”通道稳定向前传播。\n\n","categories":["AI","神经网络模型"],"tags":["ResNet"]},{"title":"PositionalEncoding","url":"/2025/09/24/Embedding/PositionalEncoding/","content":"基本概念：\ntransformer 本身是序列无关模型（self-attention 只看 token 间的相似度，并没有顺序的概念），为了引入位置信息，因此才需要引入位置编码。\n\n绝对位置编码：\n例如序列中第 i 个 token w_i，随后转换为一个 d 维 embedding x_i，为了引入索引 i 这个位置信息，在计算通常会按照一定的编码规则将其转换为一个同样是 d 维的向量 p_i，将embedding与位置编码相加后（x_i + p_i）再计算该 token 对应的 Q、K、V。\n\n最常用的绝对位置编码计算方式如下：\n\n  p_{i,2t} = \\sin{\\frac{i}{10000^{\\frac{2t}{d}}}} \\\\\n  p_{i,2t+1} = \\cos{\\frac{i}{10000^{\\frac{2t}{d}}}}\n其中第一个式子为偶数分量的计算公式，第二个式子为奇数分量的计算公式。\n\n这种编码方式无法保存相对位置关系。\n\n\n相对位置编码：\nRPE 直接在 attention 分数里加上了一个与相对位置（i-j）相关的参数：\n  score(i, j) = Q_i K^T_j + b_{i-j}\n\nRoPE：\n为了能够利用上 token 之间的相对位置信息，我们假定 query 向量 q_m 和 key 向量 k_n 之间的内积可以使用一个函数 g 来表示，其输入为 embedding x_m、x_n 和它们之间的相对位置 m-n：\n\n   = g(x_m, x_n, m-n)\n这样就能够直接将相对位置信息自然嵌入到 attention score 中，我们只需要找到一个等价的位置编码方式来使得上面的等式成立即可。\n\n论文中提到的 f 和 g 形式如下：\n\n  f_q(x_m, m) = (W_q x_m)e^{im\\theta}\\\\\n  f_k(x_n, n) = (W_k x_n)e^{in\\theta}\\\\\n  g(x_m, x_n, m-n) = Re[(W_qx_m)(W_kx_n)^*e^{i(m-n)\\theta}]\n其中 * 表示复数的共轭。\n\n推导过程\n\n最终得到的结果如下：\n\n  Q^{RoPE}_m = R(m) Q_m\\\\\n  K^{RoPE}_n = R(n) K_n\\\\\n  Q^{RoPE}_mK^{RoPE}_n = Q_m R(m-n) K_n^T\n可以看到最终的 attention score 已经包含了相对位置信息。\n\n对于 d 维的 embedding 向量，可以将 embedding 向量分量两两一组分组，每组应用相同的旋转操作（2*2旋转矩阵），每组的旋转角度计算如下：\n\n  \\theta_j = 10000^{-2(j-1)/d}, j \\in [1, 2, \\cdots, d/2]\n\n","categories":["AI","Embedding"]},{"title":"GAN","url":"/2025/09/28/NeuralNetwork/GAN/","content":"（待完善）\n\n参考文章：\n\nhttps://www.cnblogs.com/wxkang/p/17133320.html（2014-2020 GAN 变体发展概述）\nhttps://cloud.tencent.com/developer/article/2351793\nhttps://zhuanlan.zhihu.com/p/628915533\n\n\n\n\n","categories":["AI","神经网络模型"],"tags":["GAN"]},{"title":"CLIP","url":"/2025/09/28/Embedding/CLIP/","content":"\n参考文章：\nhttps://zhuanlan.zhihu.com/p/12322629416\nhttps://ai-bot.cn/shot-learning/\n\n\n\n概述：\nCLIP 使用大量数据预训练好一个文本编码器和一个图像编码器，这两个编码器可以把文本和图像投影到同一个语义空间，在训练过程中，CLIP通过对比学习，会让相匹配的图像与文本的余弦相似度更高，不匹配的更低。\n在零样本学习中，我们只需用文本 prompt 定义类别或任务，CLIP 就能够提取图像和文本的对应特征，并通过余弦相似度进行匹配，从而完成分类、检索等任务，即使之前没有针对该任务进行过额外训练。\n\n零样本学习：\n零样本学习要求模型在没有见过任何目标类别样本的情况下（没有明确标注类别的样本），仍然能够进行识别和分类。\n举个例子：训练集中没有任何包含企鹅的图片，但模型通过超大量的训练数据学习到企鹅是一种鸟类、企鹅生活在寒冷的地方、企鹅是黑白色的等特征，通过组合这些特征，模型就能够把企鹅这种“没见过”的新类别与已知的特征信息联系起来，从而实现对新类别的识别。\n在上面这个例子中，我们并没有任何包含企鹅的训练样本提供给模型进行学习，但模型仍然可以通过额外的辅助信息，组合已知的类别与特征，实现对新类别的识别，这就是零样本学习。\nCLIP 就能够实现上述的零样本学习，它能够将文本和图像信息投影到同一个语义空间中，即使在训练集中并没有明确训练或标注过“企鹅”这个类别，CLIP 仍然可以通过文本编码器，将与“企鹅”相关的文本投影到语义空间，生成的向量中包含了“鸟”、”黑白“、”冰雪背景“等语义信息。同时它也会通过图像编码器，提取出图像中的各种特征，生成另一个向量。最后让二者在同一个语义空间中进行匹配，匹配度高的即为新类别“企鹅”。\n\nCLIP Text Encoder：\n与标准的 Transformer Encoder 基本一致，输入一串文本 prompt，输出一个固定维度的 embedding，包含整句话的上下文信息，并能够与图像 embedding 对齐。\n与标准的 Transformer Encoder 不同的是，CLIP 默认取 [EOS] token（句子末尾标注）的 embedding 作为整句话的语义表示，且在计算损失函数进行优化时也只计算 [EOS] token，这样模型会默认训练成将整句话的语义信息压缩到 [EOS] token，避免了额外的池化操作。\n\n训练方法：\n\n如图所示，CLIP 同时训练一个文本编码器和一个图像编码器，输入文本-图像对，对它们进行编码并计算余弦相似度，越匹配的文本-图像对，余弦相似度应该越高，反之则必须越低，根据计算的结果与真实信息，继续更新优化两个编码器。使用超大量的数据反复执行上述操作，最终得到 CLIP 的文本编码器和图像编码器。（一般直接使用 OpenAI 等大公司预训练好的 CLIP 编码器即可）\n\n应用空间：\nCLIP 能够将文本和图像信息投影到同一个语义空间中，从而让文本信息和图像信息能够互相匹配，或是使用文本信息引导图像处理：\n图文检索：输入文本，找到最相似的图像，反之也可以实现，这也是 CLIP 的原生任务\n零样本图像分类：给定一张图片和一组类别的文本 prompt，看哪个文本更匹配\n图像聚类/语义检索：用 CLIP 提取图像 embedding，在语义空间中进行聚类或检索\n文本引导图像生成：如 Stable Diffusion 等都使用 CLIP 的 encoder 作为 prompt 的编码器\n\n\n\n","categories":["AI","Embedding"]},{"title":"LoRA","url":"/2025/09/28/Finetune/LoRA/","content":"\n参考文章：\nhttps://www.51cto.com/aigc/6010.html\nhttps://zhuanlan.zhihu.com/p/646791309\n\n\n\n背景\n大模型动辄几亿到几千亿的参数，如果针对每个任务都去进行全微调，既浪费存储空间又浪费微调时间。\n\n基本原理\n在微调过程中模型加载预训练参数 \\Phi_0 进行初始化，参数更新后变为 \\Phi_0 + \\Delta \\Phi，这种微调方式的参数增量 \\Delta \\Phi 的维度和预训练参数 \\Delta \\Phi 是一样的，微调所需的资源很多。\n研究者认为能用更少的参数表示上述要学习的参数增量 \\Delta\\Phi = \\Delta\\Phi(\\Theta)，其中 |\\Theta| \\ll |\\Phi_0|，因此原先寻找 \\Delta\\Phi 的优化目标变为寻找 \\Theta。\nLoRA就是使用一个低秩矩阵来编码 \\Delta\\Phi:\n  W_0 + \\Delta W = W_0 + BA, B \\in \\mathbb R^{d \\times r}, A \\in \\mathbb R^{r \\times k}, r \\ll min(d, k)\n\n参数更新过程\n本质上，LoRA 仍然使用反向传播来更新参数，但只针对新增的低秩矩阵 A 和 B，而保持原始权重 W 冻结。\n初始化：\nW 使用预训练模型权重，并且禁止更新\nA 用小的随机高斯分布初始化\nB 初始化为全零，确保训练开始时 \\Delta W = 0，避免干扰原始模型\n\n\n前向传播：\n输入数据 X，通过调整后的权重计算输出\n计算损失函数 L\n\n\n反向传播：\n计算损失 L 对 A 和 B 的梯度\n不计算 W 的梯度\n\n\n参数更新：\n使用优化器（如Adam）更新\n\n\n迭代优化：\n重复前面的操作，直到损失收敛或达到训练轮次\n训练完成后，A 和 B 捕捉了微调的调整信息\n\n\n\nLoRA 在 Transformer 上的应用注意力层\n对于注意力层，LoRA 最优先应用于 W_q 和 W_v：\n调整 W_q 可以改变模型关注哪些信息\n调整 W_v 可以影响模型如何编码关注的信息\n\n\n其次可以应用于 W_o\n原论文的结论是：仅微调 W_q 和 W_v 即可达到全微调效果的 90% 以上，添加 W_o 的 LoRA 对性能提升有限（&lt;2%）。\n\n前馈层\n前馈层包含两个线性变换，其中 W_1 负责升维，W_2 负责降维，二者都可以使用 LoRA 进行微调。\n\n不推荐使用 LoRA 的层\n嵌入层（Embedding）参数量大但微调收益低\nLayerNorm/Bias 参数少，可以直接全微调，无需 LoRA\n\n实际配置建议：\n小模型（如 BERT）：\nLoRA 目标层：仅 W_q 和 W_v\nrank（r）：8 - 16\n\n\n大模型（如 GPT-3）\nLoRA 目标层：W_q、W_v、FFN 的 W_1\nrank：32-64\n\n\n复杂生成任务：\nLoRA 目标层：所有注意力矩阵 + FFN\nrank：64+\n\n\n具体应用见参考文章1\n\n","categories":["AI","Finetune"]},{"title":"Stable Diffusion","url":"/2025/09/28/NeuralNetwork/DiffusionModel/","content":"\n参考文章：\nhttps://jalammar.github.io/illustrated-stable-diffusion/\nhttps://zhuanlan.zhihu.com/p/624221952\n\n\n\n逐层剖析 Stable Diffusion\n\n从宏观角度看，Stable Diffusion 主要包含两部分：一个文本解释器，一个图像生成器，用户向文本解释器中输入一串文本，经过文本解释器将其投影到特定的语义空间，随后传递给图像生成器中引导图像生成，最终输出一张生成图像。\n前面提到的文本解释器本质就是一个 Transformer Encoder，将文本中的词 token 投影到语义空间，得到对应的 embedding。\n更进一步，图像生成器主要包含两个阶段：\nImage Information Creator：这部分是 Stable Diffusion 性能超越其他模型的关键。这部分通常会执行若干步来生成图像信息，执行的步数作为可调参数，通常设置为 50 或 100。\nImage Decoder：这部分会根据 Image Information Creator 传递来的图像信息绘制图像，只在最后阶段执行一次。\n\n\n上述三个模块就是 Stable Diffusion 的主要构成，具体如下：\nText Encoder：CLIP Text Encoder\ninput：一串文本\noutput：若个干 token embedding 向量，每个向量 768 维\n\n\nImage Information Creator：U-Net + Scheduler\ninput：文本 embedding 向量、一张初始噪声图像\noutput：处理后的噪声图像信息\n\n\nImage Decoder：Autoencoder Decoder\ninput：处理后的噪声图像信息\noutput：生成图像\n\n\n\n\n\nHow Diffusion works\n扩散模型生成图像的本质就是预测噪声，通过对原始噪声图像进行逐步去噪得到生成图像。\n\n噪声预测\n\n对于训练集，我们可以通过对不同图片加噪声得到，我们假设 T = 0 时（图中 ammount = 0），图像为原始图像，T = t 时，图像经过了 t 轮加噪，每次加噪都是对原始噪声进行随机采样（实际训练时，为了保证高效，我们只对原始噪声采样一次，加噪时噪声项会乘一个系数，该系数与时刻 T 有关，具体推导过程见参考文章 2）。\n有了这个训练集，我们就可以训练一个网络（U-Net）来预测噪声，输入加噪后的图像和时刻 t，输出预测噪声，计算预测噪声与原始噪声的 loss，并反向传播更新网络，最终得到一个功能强大的噪声预测器。\n\n去噪绘制\n\n有了上面训练好的噪声预测器，我们就可以从一个噪声图像经过 T 步去噪，逐步接近最终生成的图像。但是这样生成的图像无法通过文本控制，其完全取决于训练集的图像，由训练集图像的各种特征组合而成。\n\nDiffusion on Latent Space\n为了加速图像的生成过程，Stable Diffusion 并没有选择直接在像素空间进行生成，而是先投影到 Latent Space 再进行扩散，相当于生成图像的压缩版本。\n这种压缩通过一个 autoencoder 实现，待去噪图像输入后会先被 autoencoder 投影到 latent space，在 latent sapce 去噪后，再通过 autoencoder 的 decoder 投影回像素空间，得到生成图像。这个 decoder 就是前文所提到的 Image Decoder。\n\n文本控制\n\n为了能够让文本控制图像生成过程，我们还需要向 Denoising U-Net 中输入文本信息，输入前需要通过 CLIP Text Encoder 转换为 embedding 向量。\n\n\n\n未引入文本控制的 Denoising U-Net 大致结构如图所示，其由若干层 ResNet 组成，每个 ResNet 的输出都是对应时刻去噪后的结果。\n\n\n\n为了引入跨模态的文本控制，很自然地想到可以在每个 ResNet 层后加入一个 Cross-Attention 层，由图像隐空间表示来提供 Query 向量（图像特征），文本信息提供 Key 和 Value。需要注意的是，原先的残差连接是从每个 ResNet 的输出直接连接到最后，加入了 Attention 层后，对应时刻的去噪结果是 Attention 层的输出，因此残差连接需要从每一个 Attention 层的输出开始。\n\n","categories":["AI","神经网络模型"],"tags":["Diffusion Model"]},{"title":"U-Net","url":"/2025/09/28/NeuralNetwork/U-NET/","content":"（待完善）\n","categories":["AI","神经网络模型"],"tags":["U-Net"]},{"title":"OpenDR","url":"/2025/09/29/DiffenetiableRendering/OpenDR/","content":"（待完善）\n\n参考文章：\nhttps://zhuanlan.zhihu.com/p/584510853\n\n\n\n概述：\n可微渲染需要以下两个资源：\n参考图片（Ground Truth）：二维目标图片，可微渲染的目标就是使渲染的结果无限接近这张参考图片。\n初始场景参数：可以是一个大致接近于真实场景的粗略的三维场景，也可以是完全随机的三维场景。\n\n\n其过程包含以下几步：\n渲染器根据当前场景参数渲染出一张图片\n将这场渲染出的图片与 Ground Truth 对比，可以得到一个 loss。\n将这个 loss 得到的梯度反向传播到前面的场景参数中，优化场景参数，使得渲染出的结果越来越接近 GT。\n重复前三步，直到渲染出的图片与 GT 相差无几。\n\n\n总结起来，可微渲染其实就包含两个部分：\n正向渲染：根据当前的场景参数渲染出一张二位图片。\n反向传播：正向渲染后，需要根据渲染结果和 GT 计算得到 loss，再将梯度反向传播到场景参数中，实现优化。\n\n\n但对于传统的渲染管线，我们是不可能做到反向传递梯度的。\n\n正向渲染\n我们将正向渲染流程看做一个函数 f(\\Theta)，其中 \\Theta 是用于渲染的所有的三维场景信息：\nAppearance（A）：顶点的外表信息，是映射纹理和逐顶点亮度的乘积（考虑纹理和光照的结果）\nGeometry（V）：顶点的几何信息\nCamera（C）：相机的参数信息\n\n\n中间信息：\nProjection coordinates（U）：顶点经过坐标变换后在屏幕空间中的坐标\n\n\n那么 \\Theta 可以表示为：\\Theta = {A, V, C}。\n\n反向求导\n可微渲染的目标就是将梯度反向传播到 A、V、C 中，从而优化场景参数，其中各个参数之间的联系如下：\n\n可导性分析\n\\frac{\\partial A}{\\partial V}：顶点外表信息对顶点位置的导数，该映射关系是用户自己定义的，只要用户定义的函数可导，这个导数就存在。\n\\frac{\\partial U}{\\partial V}：顶点的屏幕坐标对场景坐标的导数，这里的运算只涉及矩阵运算，而矩阵运算天然可导，因此这个导数是存在的。\n\\frac{\\partial U}{\\partial C}：顶点的屏幕坐标对相机参数的导数，与前者相同，同样是矩阵运算。\n\\frac{\\partial f}{\\partial A}:屏幕像素颜色对顶点外表信息的导数。在光栅化时，片元的属性值（如颜色）是由它所在的三角形的三个顶点进行插值得到的，我们知道三角形内一点的重心坐标是P = \\lambda_1 P_1 + \\lambda_2 P_2 + \\lambda_3 P_3，那么该片元的属性值为 f(P) = \\lambda_1 f(P_1） + \\lambda_2 f(P_2) + \\lambda_3 f(P_3)，因此屏幕像素颜色对顶点颜色的导数即为该像素对应的可见片段在其三角形的重心坐标。\n\\frac{\\partial f}{\\partial U}：屏幕像素颜色对顶点屏幕坐标的导数。这个阶段会经过光栅化，因此这类函数是不连续的，从而导致该函数不可导。举个例子：如果该像素所在的三角形的三个顶点颜色都是白色，则该像素的颜色根据插值得到的颜色也是白色，此时若移动某一个顶点，在某一个时刻其颜色突变为黑色，则插值出来的像素颜色也会发生突变，因此该函数并不连续。\n因此可微渲染的关键就在于如何去表示像素颜色与顶点屏幕坐标的关系。\n\nOpenDR"}]