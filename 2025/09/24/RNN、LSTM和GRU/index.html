<!DOCTYPE html><html lang="cn" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>RNN、LSTM和GRU | Kamikit's Blog</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/font/BenderLight.woff2"><link rel="preload" as="font" crossorigin="anonymous" href="/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/font/Bender.ttf"), url("/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><meta name="page-config" content="{&quot;code_fold&quot;:null}"><script class="pjax-js">var config = {"root":"/","code_fold":15,"search":{"preload":false,"activeHolder":"键入以继续","blurHolder":"数据检索","noResult":"无 $0 相关数据"},"code":{"codeInfo":"$0 - $1 行","copy":"复制"}};
var page_config = {
  code_fold: null
};
function updatePageConfig() {
  var newPageConfig = document.querySelector('meta[name="page-config"]');
  if (newPageConfig) {
    page_config = JSON.parse(newPageConfig.content);
  }
}
document.addEventListener('pjax:complete', function() { updatePageConfig(); });
updatePageConfig();</script><link type="text/css" rel="stylesheet" href="/lib/encrypt/hbe.style.css"><script src="//unpkg.com/mermaid@10.5.0/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.woff2") format('woff2');
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
 --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
 --light-background: url('/img/bk.jpg');
 --theme-encrypt-confirm: '确认'
}</style><script defer src="/js/arknights.js"></script><script defer src="/js/search.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/lib/encrypt/hbe.js"></script><script async src="/js/pjax.js"></script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.vercount','meta[name=page-config]'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><script class="pjax-js">reset= () => {document.querySelector('.lg-container')?.remove()
const postBg = document.querySelector('#post-bg');
if (postBg) lightGallery(postBg, {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.vercount','meta[name=page-config]'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 8.0.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="数据检索" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>RNN、LSTM和GRU</h1><div id="post-info"><span>文章发布时间: <div class="control"><time datetime="2025-09-24T08:22:25.000Z" id="date"> 2025-09-24</time></div></span><br><span>最后更新时间: <div class="control"><time datetime="2025-09-24T08:23:05.539Z" id="updated"> 2025-09-24</time></div></span></div></div><hr><div id="post-content"><ul>
<li>参考文章：<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/123211148">https://zhuanlan.zhihu.com/p/123211148</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/491564016">https://zhuanlan.zhihu.com/p/491564016</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32085405">https://zhuanlan.zhihu.com/p/32085405</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32481747">https://zhuanlan.zhihu.com/p/32481747</a></li>
<li><a target="_blank" rel="noopener" href="https://zybuluo.com/hanbingtao/note/541458">https://zybuluo.com/hanbingtao/note/541458</a></li>
<li><a target="_blank" rel="noopener" href="https://zybuluo.com/hanbingtao/note/581764">https://zybuluo.com/hanbingtao/note/581764</a></li>
</ul>
</li>
</ul>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li>对于同一个单词，它在不同句子中的含义可能不同，传统的神经网络模型无法提取其在序列中的特征，预测的准确程度仅取决于训练集中label的数量。</li>
</ul>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p class='item-img' data-src='/1.jpg'><img src="/1.jpg"></p>
<ul>
<li><p>输入层：X 是一个向量，表示输入信息，与隐藏层之间不是全连接的，而是按照时刻与隐藏层对齐连接（例如输入一段英文，每个单词对应的向量与隐藏层对齐连接）</p>
</li>
<li><p>隐藏层：S 是一个向量，表示隐藏层的值（节点数与 S 的维度相同）</p>
</li>
<li><p>输出层：O 是一个向量，表示输出层的值</p>
</li>
<li><p>模型参数：U 是输入层到隐藏层的权重矩阵，V 是隐藏层到输出层的权重矩阵，W 是隐藏层到隐藏层的权重矩阵，其中 V 和 W 都是全连接的，且各个时刻的 U、V 和 W 都共享各自的参数</p>
</li>
<li><p><strong>计算过程</strong>：<br>$$<br>  S_t &#x3D; f(U \cdot X_t + W \cdot S_{t-1})\<br>  O_t &#x3D; g(V \cdot S_t)<br>$$</p>
<ul>
<li>其中 f 和 g 为激活函数</li>
</ul>
</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>由于整个计算过程都是顺序的，每次输入一个 embedding 顺序计算，后面得到的计算结果包含序列前面的 token 信息，但是前面得到的计算结果并不包含序列后面的 token 信息，因此表达能力有限。</li>
<li>这种最基础的 RNN 存在梯度消失和梯度爆炸的问题，因此这种最朴素的 RNN 通常不会使用。</li>
</ul>
<h4 id="RNN-梯度爆炸和梯度消失问题"><a href="#RNN-梯度爆炸和梯度消失问题" class="headerlink" title="RNN 梯度爆炸和梯度消失问题"></a>RNN 梯度爆炸和梯度消失问题</h4><ul>
<li>简单来说，RNN 梯度消失就是在反向传播过程中，梯度的计算中包含了矩阵的连乘，如果这些矩阵的特征值小于 1，则梯度会指数衰减，最终导致靠近输入层的梯度几乎为 0，参数得不到更新。梯度爆炸与梯度消失类似，如果这些矩阵的特征值大于 1，则梯度会指数增长，从而导致模型更新过大，无法收敛。</li>
<li>具体解析如下：<ul>
<li>在 RNN 中，更新隐状态的矩阵 $W_h$ 参与了所有时刻隐状态的更新，因此损失对其的梯度为：<br>  $$<br>  \frac{\partial L}{\partial W_h} &#x3D; \sum_{t &#x3D; 1}^T\frac{\partial L}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_h}<br>  $$</li>
<li>其中 $\frac{\partial h_t}{\partial W_h}$ 可以直接求导得到。</li>
<li>对于前项 $\frac{\partial L}{\partial h_t}$，可以通过递推公式求得（默认激活函数为 tanh）：<br>  $$<br>  \frac{\partial L}{\partial h_t} &#x3D; \sum_{k &#x3D; t}^T(\frac{\partial L}{\partial h_k} \prod_{j &#x3D; t + 1}^k W_h^T \cdot diag(1 - h_j^2))<br>  $$</li>
<li>可以看到，这一部分的梯度包含了矩阵连乘，这也就是梯度爆炸和梯度消失的根源。</li>
<li>该递推式可以如下理解：假设当前时刻为 $t$，那么 $h_t$ 会参与 $t$ 时刻之后的所有隐状态更新，因此 $t$ 时刻的梯度需要对后续时刻的梯度求和得到。递推式的边界条件即为最后时刻的梯度，可以根据损失函数直接求导得到。</li>
</ul>
</li>
</ul>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul>
<li>为了解决 RNN 的梯度消失和梯度爆炸问题，提出了 LSTM（Long Short Term Memory Network）。</li>
<li>原先的 RNN 隐藏层只有一个状态 h（hidden），其对于短期的输入非常敏感，因此 LSTM 的核心思路就是再添加一个隐藏层状态 c（cell），其在向前传递的过程中变化很慢，用于保存长期的状态。</li>
<li>LSTM 引入了三个门控单元，用于控制 c 的状态与其对输出的贡献。**遗忘门（forget gate）**决定了上一时刻的 cell state $c_{t-1}$有多少会保留到当前时刻 $c_t$；**输入门（input gate）**决定了当前时刻网络的输入 $x_t$ 有多少会保存到 $c_t$；**输出门（output gate）**控制 $c_t$ 有多少输出到 $h_t$。</li>
<li>其具体结构与计算过程如下图所示：<br class='item-img' data-src='/2.png'><img src="/2.png"></li>
<li>首先 LSTM 会先用当前输入 $x^t$ 和上一个状态传递下来的 $h^{t-1}$ 拼接并计算得到 4 个状态，其中 $z$ 是输入，$z^f, z^i, z^o$ 是三个门控单元，分别是遗忘门、输入门和输出门。<br>$$<br>  z &#x3D; tanh(W \cdot [x_t, h_{t-1}] + b) \<br>  z^f &#x3D; sigmoid(W_f \cdot [x_t, h_{t-1}] + b_f) \<br>  z^i &#x3D; sigmoid(W_i \cdot [x_t, h_{t-1}] + b_i) \<br>  z^o &#x3D; sigmoid(W_o \cdot [x_t, h_{t-1}] + b_o) \<br>$$</li>
<li>如上图所示，LSTM 的内部计算主要包含三个阶段：<ul>
<li>忘记阶段：该阶段主要是对上一时刻的 cell 进行<strong>选择性遗忘</strong>，舍弃不重要信息，保留重要信息，这个过程由忘记门控 $z^f$ 控制。</li>
<li>选择记忆阶段：该阶段会对当前时刻输入 $z$ 进行<strong>选择性记忆</strong>，这个过程由输入门控 $z^i$ 控制。<br>  $$<br>  c_t &#x3D; z^f \odot c_{t-1} + z^i \odot z<br>  $$</li>
<li>输出阶段：该阶段决定哪些 cell 的状态会作为当前状态输出，这个过程由输出门控 $z^o$ 进行控制。<br>  $$<br>  h_t &#x3D; z^o \odot tanh(c_t)<br>  $$</li>
</ul>
</li>
<li>最终网络的输出与 RNN 类似：<br>$$<br>  y_t &#x3D; sigmoid(W^\prime \cdot h_t)<br>$$</li>
</ul>
<h4 id="为何-LSTM-能够解决-RNN-的梯度爆炸和梯度消失问题"><a href="#为何-LSTM-能够解决-RNN-的梯度爆炸和梯度消失问题" class="headerlink" title="为何 LSTM 能够解决 RNN 的梯度爆炸和梯度消失问题"></a>为何 LSTM 能够解决 RNN 的梯度爆炸和梯度消失问题</h4><ul>
<li>RNN 的梯度需要通过隐状态传递，从而出现了矩阵连乘形式，而 LSTM 通过引入 cell state 和遗忘门控使得梯度可以通过这条稳定的路径传递，只要有了这条梯度不会消失和爆炸的传递路径，我们就可以有效地更新模型的参数。</li>
<li>LSTM 的 cell state 更新如下：<br>$$<br>  c_t &#x3D; f_t \odot c_{t-1} + i_t \odot z<br>$$</li>
<li>对于 RNN 的隐状态更新，$\frac{\partial h_t}{\partial h_{t-1}}$ 可能与激活函数的导数相关，无法保证其稳定在 1 附近。</li>
<li>LSTM 通过引入遗忘门控来保证 cell state 这条路径可以稳定传递梯度，即：<br>$$<br>  \frac{\partial c_t}{\partial c_{t-1}} &#x3D; f_t \approx 1<br>$$</li>
</ul>
<h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>相较于最朴素的 RNN，LSTM 引入了很多内容，导致参数变多，使得训练难度加大了很多，因此通常会使用效果和 LSTM 效果相当但参数更少的 GRU 来构建大模型。</li>
</ul>
<h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><ul>
<li><p>GRU 的结构如图所示：<br class='item-img' data-src='/3.jpg'><img src="/3.jpg"></p>
</li>
<li><p>GRU 引入了两个门控单元，其中 r 为重置门（reset gate），z 为更新门（update gate）：<br>$$<br>  r_t &#x3D; sigmoid(W_r \cdot [x_t, h_{t-1}])<br>  z_t &#x3D; sigmoid(W_z \cdot [x_t, h_{t-1}])<br>$$</p>
</li>
<li><p>首先 GRU 会通过重置门将新的输入信息与前面的记忆相结合，具体公式如下：<br>$$<br>  h^\prime &#x3D; tanh(W \cdot [x_t, r_t \odot h_{t-1}])<br>$$</p>
</li>
<li><p>$r_t$ 越小，则前面的记忆占比越小，表示上一时刻的记忆需要丢弃的越多；反之，$r_t$ 越大，则说明上一时刻需要记住的越多。因此重置门是对前一时刻的记忆进行一定的重置，有助于提取序列中短期的关系。</p>
</li>
<li><p>随后 GRU 会进行遗忘和选择记忆，即更新记忆，具体公式如下：<br>$$<br>  h_t &#x3D; (1 - z_t) \odot h_{t-1} + z_t \odot h^\prime<br>$$</p>
</li>
<li><p>$(1 - z_t) \odot h_{t-1}$ 表示对原本隐藏状态的选择性遗忘，$z_t$ 越大，表示遗忘的更多，剔除记忆中不重要的信息。</p>
</li>
<li><p>$z_t \odot h^\prime$表示对当前节点的信息 $h^\prime$ 进行选择性记忆，$z_t$ 越大，表示保存到记忆中的信息越多。</p>
</li>
</ul>
<div id="paginator"></div></div><div id="post-footer"><div id="pages" style="justify-content: flex-end"><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2025/09/24/CNN/">CNN 上一篇 →</a></div></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="回到顶部" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="文章目录">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="切换主题"></a><a onclick="BgmControl()"><svg id="bgm-control" viewBox="0 0 30 34" fill="#18d1ff" style="width: 24px; transition: transform .3s;margin-top: 4px"><path d="M25.998 23.422V11.29h3.999v12.132h-3.999zM19.497 6.234h4.001v22.243h-4.001V6.234zM12.998.867h4v32.978h-4V.867zm-6.5 5.367h4.001v22.243H6.498V6.234zm-6.5 5.056h4v12.132h-4V11.29z"></path></svg><audio id="bgm" src="/audio/bgm.mp3" autoplay loop crossorigin="anonymous"> </audio></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo" style="margin:0;border-radius:0;"></a><h1 id="Dr"><a href="/">Kamikit</a></h1><div id="description"><p></p></div></div><div id="aside-block"><div id="toc-div"><h1>目录</h1><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN"><span class="toc-number">1.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">模型结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">1.3.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RNN-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98"><span class="toc-number">1.4.</span> <span class="toc-text">RNN 梯度爆炸和梯度消失问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM"><span class="toc-number">2.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95-LSTM-%E8%83%BD%E5%A4%9F%E8%A7%A3%E5%86%B3-RNN-%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.</span> <span class="toc-text">为何 LSTM 能够解决 RNN 的梯度爆炸和梯度消失问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-1"><span class="toc-number">2.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRU"><span class="toc-number">3.</span> <span class="toc-text">GRU</span></a></li></ol></div></div><footer><nobr>构建自 <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> 使用主题 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> 主题作者 <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>